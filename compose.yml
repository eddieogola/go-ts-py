services:
  py-ai:
    build:
      context: ./py-ai
      dockerfile: Dockerfile
    environment:
      - BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
      - MODEL_NAME=ai/gemma3
    develop:
      watch:
        - path: ./py-ai/
          target: /app/
          action: sync
          ignore:
            - "**/__pycache__/**"
            - "**/*.pyc"
            - "**/.venv/**"
        - path: ./py-ai/pyproject.toml
          action: restart
    models:
      - llm
  ts-ai:
    build:
      context: ./ts-ai
      dockerfile: Dockerfile
    environment:
      - BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
      - MODEL_NAME=ai/gemma3
    develop:
      watch:
        - path: ./ts-ai/
          target: /app/
          action: sync
        - path: ./ts-ai/package.json
          action: rebuild

    models:
      - llm
  go-ai:
    build:
      context: ./go-ai
      dockerfile: Dockerfile
    environment:
      - BASE_URL=http://model-runner.docker.internal/engines/llama.cpp/v1
      - MODEL_NAME=ai/gemma3
    develop:
      watch:
        - path: ./go-ai/
          target: /app/
          action: sync
        - path: ./go-ai/go.mod
          action: restart
    models:
      - llm

models:
  llm:
    model: ai/gemma3
